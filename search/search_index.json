{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"<p>These pages detail the use of LEAP's computational and data resources, including LEAP Pangeo. They are focused on LEAP-specific information as much as possible. The Pangeo community and the wider internet may be useful in figuring out how to accomplish specific tasks. You can always ask for help.</p> <ul> <li>The Introduction explains what the resources are and how to gain access to them.</li> <li>The Data section explains where data lives and what data is publicaly available.</li> <li>The Compute section explains how to leverage LEAP's computing resources to solve your problems</li> <li>The Education section is intended for people teaching classes, running bootcamps, etc.</li> <li>The Support section contains answers to frequently asked questions including how to get more help.</li> <li>The Reference section contains nitty-gritty details.</li> </ul>"},{"location":"compute/","title":"Using LEAP's computing resources","text":"<p>These pages describe how to use LEAP's computing resources to solve your problems, including a range of ways to access the hub, how to request the appropriate computing resources, and how to install software you might need.</p>"},{"location":"compute/compute_scaling/","title":"Compute Scaling","text":"<p>When launching a server on the LEAP JupyterHub, you'll be asked to select a compute configuration. This guide helps you choose the right image and hardware resources (RAM and CPU/GPU) for your workflow.</p>"},{"location":"compute/compute_scaling/#image-types","title":"Image Types","text":"<p>Each image contains a different set of pre-installed software packages. Choose the image that fits your computing needs:</p> Image Name Use When You Need... Base Pangeo Notebook General scientific stack (e.g., xarray, dask, matplotlib). Ideal for climate, ocean, and earth science workflows. Pangeo PyTorch ML Notebook PyTorch for machine learning. Runs on CPU or GPU depending on your hardware choice. Pangeo TensorFlow ML Notebook TensorFlow for machine learning. Runs on CPU or GPU depending on your hardware choice. Other... Enter a custom image URL"},{"location":"compute/compute_scaling/#resource-options","title":"Resource Options","text":"<p>Choose a CPU/GPU configuration based on the size of your data and the complexity of your tasks.</p>"},{"location":"compute/compute_scaling/#cpu","title":"CPU","text":"<p>Use this for data exploration, lightweight model runs, or debugging.</p> Option Use Case ~8 GB, ~1.0 CPU Small notebooks, light plotting, CSVs or small NetCDF. ~16\u201364 GB, ~2\u20138 CPU Medium-sized xarray/dask workloads, ML prototyping. ~128 GB, ~16 CPU Large simulations, ensemble runs, or parallel workflows."},{"location":"compute/compute_scaling/#gpu","title":"GPU","text":"<p>Use this for training deep learning models or doing heavy inference.</p> <ul> <li>NVIDIA Tesla T4, 24GB RAM, 8 CPUs</li> <li>Compatible with all images</li> <li>Greatly accelerates TensorFlow and PyTorch training</li> </ul>"},{"location":"compute/compute_scaling/#why-not-use-gpu-by-default","title":"Why not use GPU by default?","text":"<p>While GPU can accelerate certain workloads, it's not always the best choice for the following reasons:</p> <ul> <li>Most tasks don't benefit: Plotting, pandas/xarray analysis, or basic modeling run just as fast (or faster) on CPUs.</li> <li>Shared, limited resources: GPUs are a shared resource across LEAP. Using them when not needed can block others who rely on them for large-scale work.</li> <li>More costly: GPUs cost significantly more than CPU resources.</li> </ul> <p>Note</p> <p>Please use GPUs only if your workflow truly needs it.</p>"},{"location":"compute/compute_scaling/#how-to-choose","title":"How to choose:","text":"<p>Here is a simplified guide on how to choose the appropriate image and compute configuration:;</p> Scenario Recommended Setup Editing a notebook, small CSVs Base Pangeo Notebook + 8 GB CPU Plotting large NetCDF files with xarray Base Pangeo Notebook + 16\u201364 GB CPU Visualizing high-resolution model outputs Base Pangeo Notebook + 16\u201364 GB CPU Preprocessing large climate or satellite datasets Base Pangeo Notebook + 64\u2013128 GB CPU Large parallel Dask workloads Base Pangeo Notebook + 32\u2013128 GB CPU Interactive Dask dashboard or distributed workflows Base Pangeo Notebook + 32\u2013128 GB CPU Running large batch inference PyTorch/TensorFlow ML Notebook + GPU Debugging or inference on smaller models PyTorch/TensorFlow ML Notebook + CPU Training a PyTorch model PyTorch ML Notebook + GPU Running a TensorFlow model at scale TensorFlow ML Notebook + GPU Fine-tuning pre-trained deep learning models PyTorch/TensorFlow ML Notebook + GPU Hyperparameter tuning / grid search (ML) PyTorch/TensorFlow ML Notebook + GPU or Devs Only Generating synthetic datasets or simulations Base Pangeo Notebook + 16\u2013128 GB CPU <p>Tip</p> <p>If you are not sure which one to pick, then start with the Base Pangeo Notebook + 8-16 GB CPU. You can always stop your server and restart with a different configuration</p>"},{"location":"compute/dask/","title":"Dask Guide","text":"<p>Dask can be deployed on distributed infrastructure, such as an HPC system or a cloud computing system. There is a growing ecosystem of Dask deployment projects that facilitate easy deployment and scaling of Dask clusters on a wide variety of computing systems.</p> <p>Within LEAP JupyterHub, we can use the <code>dask-gateway</code> to create dask clusters.</p>"},{"location":"compute/dask/#introduction-to-dask-gateway","title":"Introduction to Dask Gateway","text":"<p>Dask Gateway helps you manage Dask clusters for multiple users in a scalable, secure, and resource-efficient way. It's designed to work well in environments like JupyterHub or shared cloud infrastructure.</p> <p>Rather than manually setting up a Dask cluster, Dask Gateway automates the process, letting you spin up resources on-demand without worrying about underlying infrastructure details.</p>"},{"location":"compute/dask/#when-to-use-dask-gateway","title":"When to use Dask Gateway:","text":"<p>Dask Gateway is useful when:</p> <ul> <li>You need to run Dask jobs on a shared cluster.</li> <li>You want to scale your computation dynamically, particularly in cloud or HPC environments.</li> <li>You prefer not to manage the underlying infrastructure manually (e.g., node management, worker allocation).</li> </ul>"},{"location":"compute/dask/#quick-start-notebook","title":"Quick start (Notebook)","text":"<p>This sequence connects to Dask Gateway, starts a cluster, attaches a client, and scales workers as needed.</p> <pre><code>from dask_gateway import Gateway\n\n# Connect to Gateway (preconfigured on the Hub)\ngateway = Gateway()\n\n# Inspect available cluster options (cores, memory, image, env vars)\ngateway.cluster_options()\n\n# Create a cluster\ncluster = gateway.new_cluster()\ncluster\n\n# Connect a Dask client\nclient = cluster.get_client()\nclient\n\n# Scale on demand (fixed size or adaptively)\ncluster.scale(4)  # fixed size: set an explicit number of workers\n\ncluster.adapt(\n    minimum=1, maximum=20\n)  # adaptively: autoscale between a min/max based on workload\n\n# Link to the Dask dashboard for task graphs, memory, and progress\nclient.dashboard_link\n</code></pre> <p>!!! tip:</p> <pre><code>The **Dask dashboard** visualizes work in real time (graphs, progress bars, worker memory). It's the best place to debug performance.\n</code></pre>"},{"location":"compute/dask/#example-daskarray-workload","title":"Example: dask.array workload","text":"<p>This example shows how Dask splits a big array into chunks, builds a task graph, and only runs it when asked (lazy computation).</p> <pre><code>import dask.array as da\n\n# 20k x 20k random array, chunked into 1000 x 1000 tiles\n# Chunking controls parallelism and memory per task\nx = da.random.random((20_000, 20_000), chunks=(1_000, 1_000))\ny = x + x.T\nz = y[::2, 5000:].mean(axis=1)\n\n# Compute once workers are ready (executes lazily until this line)\nz.compute()\n</code></pre> <p>If the runtime is too long, then we can scale up:</p> <pre><code>cluster.scale(8)  # add workers, then recompute\nz.compute()\n</code></pre>"},{"location":"compute/dask/#example-xarray-zarr-gcs","title":"Example: xarray + Zarr (GCS)","text":"<p>This example opens a remote Zarr dataset stored in Google Cloud Storage (GCS), builds a lazy analysis pipeline, and then executes it on a Dask cluster.</p> <pre><code>import xarray as xr\n\n# Zarr example on GCS\nds = xr.open_dataset(\n    \"gs://cmip6/CMIP6/HighResMIP/MOHC/HadGEM3-GC31-HM/highresSST-present/r1i1p1f1/3hr/tas/gn/v20170831/\",\n    engine=\"zarr\",\n    chunks={},\n)\n\n# Inspect and adjust chunking if needed\nds.chunks\nds = ds.chunk({\"time\": 240, \"lat\": 256, \"lon\": 256})\n\n# Build a lazy pipeline: Area-weighted global mean, then a rolling mean along time\nweights = np.cos(np.deg2rad(ds.lat))\ntas_global = ds.tas.weighted(weights).mean((\"lat\", \"lon\"))\ntas_rolled = tas_global.rolling(time=24, center=True, min_periods=12).mean()\n\n# Trigger computation on the cluster\nresult = tas_rolled.compute()\nprint(result)\n</code></pre>"},{"location":"compute/dask/#dask-xarray-tips-and-tricks","title":"Dask / Xarray tips and tricks","text":"<p>The Xarray docs have a page on Xarray + dask best practices here</p>"},{"location":"compute/dask/#correct-chunk-sizes","title":"Correct chunk sizes","text":"<p>When working with Xarray and Zarr, you should aim for data chunk sizes around ~100 MB (rule of thumb: ~50\u2013250 MB). Chunk sizes that are too small can overwhelm the Dask scheduler, while chunks that are too large can cause memory issues.</p> <p>You can examine the chunk size and shape by viewing the HTML repr of an Xarray dataset in a Jupyter Notebook. If you click on the right-most database logo you should get a drop-down menu that shows the chunking information.</p> <p></p> <p>Good chunking balances memory, overhead, and parallelism:</p> <ul> <li>Aim for ~100 MB per chunk of array data (rule of thumb ~50\u2013250 MB).</li> <li>Align chunks with the access pattern (e.g., chunk along time for time-wise operations).</li> <li>Rechunk explicitly when the store\u2019s defaults aren\u2019t ideal.</li> </ul>"},{"location":"compute/dask/#delay-computation-until-write","title":"Delay computation until write","text":"<p>Xarray is great at lazy computation; it\u2019s usually possible to run multiple operations before any computation is done. Keep everything lazy (not eager!) until you finish your processing and write your data. For example, a <code>to_zarr()</code> call will trigger the computation. This can generally be accomplished by not calling <code>.load()</code>, <code>.persist()</code>, or <code>.compute()</code> during intermediate steps.</p>"},{"location":"compute/dask/#use-the-dask-dashboard","title":"Use the Dask dashboard","text":"<p>If you are using the Dask distributed scheduler you can view the Dask dashboard in a browser. This allows you to see memory usage, task progression, and a bunch of other metrics live.</p> <pre><code># Local / standalone scheduler:\nfrom distributed import Client\n\nclient = Client()\nclient\n</code></pre> <p>On Dask Gateway (recommended on the Hub), you\u2019ll typically do:</p> <pre><code># Using Gateway:\nclient = cluster.get_client()\nclient.dashboard_link\n</code></pre>"},{"location":"compute/dask/#memory-management-with-xarray-and-dask","title":"Memory Management with Xarray and Dask","text":"<p>Running out of memory (OOM) is one of the most common issues when working with climate datasets, even on 128 GB workers. Here are patterns to avoid and recommended alternatives.</p> <p>Pitfall: eager load + rechunk</p> <pre><code>import xarray as xr\n\n# This will likely OOM!\nds = xr.open_dataset(\"bigfile.nc\").load()\nds = ds.chunk({\"time\": 100})\nresult = ds.mean(\"time\").compute()\n</code></pre> <p>Here <code>.load()</code> forces the full dataset into memory before chunking.</p> <p>Better: lazy rechunk + write + reload</p> <pre><code># Open lazily with Dask\nds = xr.open_dataset(\"bigfile.nc\", chunks={})\n\n# Rechunk safely\nds_rechunked = ds.chunk({\"time\": 100})\n\n# Write to Zarr\nds_rechunked.to_zarr(\"gs://leap-persistent/username/bigfile_rechunked.zarr\")\n\n# Reopen and process\nds2 = xr.open_zarr(\"gs://leap-persistent/username/bigfile_rechunked.zarr\")\nresult = ds2.mean(\"time\").compute()\n</code></pre> <p>This keeps memory use bounded.</p>"},{"location":"compute/dask/#processing-subsets-with-isel","title":"Processing subsets with .isel()","text":"<p>For very large data, process subsets explicitly and append:</p> <pre><code>for i in range(6):  # e.g. nf = [0,1,2,3,4,5]\n    ds_sub = ds.isel(nf=[i])  # brackets preserve dimension\n    processed = my_processing(ds_sub)\n\n    processed.to_zarr(\n        \"gs://leap-persistent/username/processed.zarr\",\n        mode=\"a\",\n        append_dim=\"nf\",\n    )\n</code></pre>"},{"location":"compute/dask/#tips-to-avoid-oom","title":"Tips to avoid OOM","text":"<ul> <li>Chunk around 100 MB per task</li> <li>Stay lazy until the end of workflow</li> <li>Use the Dask dashboard to monitor memory</li> <li>Scale workers with <code>cluster.scale()</code> or <code>cluster.adapt()</code></li> <li>Write intermediates with .to_zarr() to checkpoint progress</li> </ul> <p>Tip</p> <p>If you hit repeated OOM errors, try the subset-loop approach (isel + to_zarr with mode=\"a\"). It trades some complexity for much more stability.</p>"},{"location":"compute/dask/#cleaning-up","title":"Cleaning up","text":"<p>When, make sure to shut down your Dask cluster (scheduler + workers) so the compute nodes are released back to the shared Gateway pool. This stops your resource usage and frees capacity for others. Use the following code snippet to do this:</p> <pre><code>client.close()\ncluster.shutdown()\n</code></pre>"},{"location":"compute/dask/#common-issues-and-fixes","title":"Common issues and fixes","text":"Issue Quick fixes Workers killed / OOM Increase worker memory; reduce chunk sizes; avoid creating huge numbers of tiny tasks; consider <code>ds = ds.chunk({...})</code> with larger chunks per task. Slow progress Scale up workers with <code>cluster.scale(...)</code> or use <code>cluster.adapt(minimum=..., maximum=...)</code>; rechunk for bigger tasks; use <code>persist()</code> on reused intermediates. Dataset not found / auth errors Verify the bucket/path; check credentials and permissions; confirm whether code is running on the Hub vs. local and that the selected interpreter/kernel matches. Dashboard not loading Recreate the client/cluster, then reopen <code>client.dashboard_link</code>; ensure the dashboard URL isn\u2019t blocked by network/firewall settings."},{"location":"compute/dask/#other-ways-to-run-dask","title":"Other ways to run Dask","text":"<p>While Dask is a great tool for scaling, there are other ways to run Dask on distributed systems.</p> <ul> <li>HPC (job schedulers): https://jobqueue.dask.org/</li> <li>Dask MPI: https://mpi.dask.org/</li> <li>Managed cloud: https://coiled.io/</li> <li>Kubernetes: https://kubernetes.dask.org/en/latest/</li> <li>Cloud provider SDKs: https://cloudprovider.dask.org/en/latest/</li> </ul>"},{"location":"compute/hub_access/","title":"Hub Access Overview","text":"<p>There are multiple ways to access the JupyterHub depending on your workflow preferences. You can:</p> <ul> <li>Access the Hub directly through a web browser</li> <li>Connect to the Hub using VSCode on your local machine</li> <li>Use a VSCode interface inside JupyterLab via a built-in extension</li> </ul> <p>This page can help you decide which method is right for you based on your workflow.</p> Method Best For Browser (JupyterLab) New users, teaching, notebooks, simple experiments VS Code inside the Hub Repo + notebook workflows, richer IDE, no setup Local VS Code IDE Power users who want full desktop performance &amp; features"},{"location":"compute/hub_access/#method-comparison","title":"Method Comparison","text":""},{"location":"compute/hub_access/#1-browser-jupyterlab-web-ui","title":"1. Browser (JupyterLab / Web UI)","text":"<p>Pros</p> <ul> <li>Fastest onboarding, works from any machine</li> <li>Zero local setup or install</li> <li>Guaranteed environment parity with Hub compute</li> <li>Ideal for teaching, workshops, quick data exploration</li> </ul> <p>Cons</p> <ul> <li>Not a full IDE (limited refactoring, UX, extensions)</li> <li>Multi-file repo workflows can feel clunky</li> <li>Fewer features compared to VS Code</li> </ul> <p>Use-cases</p> <ul> <li>First-time users, classes, tutorials</li> <li>Notebook-centric workflows</li> <li>Reliable &amp; consistent shared environments</li> </ul> <p>See installation instructions here: Access via Web Browser</p>"},{"location":"compute/hub_access/#2-vs-code-inside-the-hub-browser-based-vs-code-extension","title":"2. VS Code Inside the Hub (browser-based VS Code extension)","text":"<p>Pros</p> <ul> <li>Rich IDE in your browser tab</li> <li>No SSH setup or config</li> <li>Full access to your Hub environment (same kernel, filesystem, terminal)</li> <li>Easier repo navigation, Git workflows, split views, terminal + notebooks side-by-side</li> </ul> <p>Cons</p> <ul> <li>Some extensions may be disabled/unavailable</li> <li>Slight lag in slower networks</li> <li>Not as fully integrated as local VS Code</li> </ul> <p>Use-cases</p> <ul> <li>Daily IDE-like workflows on Hub compute</li> <li>Working with both notebooks and scripts in one place</li> <li>Collaborative work on shared repos</li> </ul> <p>See installation instructions here: Access VSCode inside the Hub</p>"},{"location":"compute/hub_access/#3-local-vs-code-ide-connected-to-the-hub","title":"3. Local VS Code IDE Connected to the Hub","text":"<p>Pros</p> <ul> <li>Richest developer experience (UI, extensions, keybindings)</li> <li>Ideal for long sessions and power-user workflows</li> <li>Full VS Code features, debugging, linting, Git integrations</li> </ul> <p>Cons</p> <ul> <li>Setup can be fragile (SSH, token config, port-forwarding)</li> <li>Easy to mistakenly use your local interpreter/kernel</li> <li>Credentials or data may unintentionally end up stored locally</li> <li>More complex to support in classrooms/workshops</li> </ul> <p>Use-cases</p> <ul> <li>Power users comfortable with remote dev tools</li> <li>Working on large codebases or long-running services</li> <li>Custom local workflows that the Hub can\u2019t support directly</li> </ul> <p>See installation instructions here: Access via VSCode (external)</p>"},{"location":"compute/hub_access/#installing-packages-environment-behavior","title":"Installing Packages &amp; Environment Behavior","text":"Behavior Browser / VS Code in Hub Local VS Code IDE Where packages install On the Hub compute Depends on interpreter\u2014can be local or remote Persistence across server restarts Yes, if installed in <code>$HOME</code> or Conda env Same, if using remote interpreter Common pitfall None Accidentally installing locally instead of on the Hub <p>Note</p> <p>Go to Managing Software for more guidance on installing packages.</p>"},{"location":"compute/hub_access/#security-data-movement","title":"Security &amp; Data Movement","text":"<ul> <li>Browser and in-Hub VS Code: data stays on the Hub unless explicitly downloaded</li> <li>Local VS Code IDE: easier to accidentally sync data to your local machine or expose secrets via cached credentials</li> </ul> <p>Tip</p> <p>You can switch between methods at any time - no need to stick to one - All methods access the same files in your Hub home directory</p>"},{"location":"compute/hub_browser_access/","title":"Accessing the Hub via Web Browser (JupyterLab)","text":"<p>Accessing the LEAP JupyterHub via your browser is the simplest and most reliable method, especially for new users, students, and notebook-based workflows.</p> <p>How to Launch:</p> <ol> <li>Go to the LEAP JupyterHub browser</li> <li>Login with your institutional credentials</li> <li>This should bring you to a main menu where you can choose your cloud-compute resources and environment</li> </ol> <p></p> <p>For more information on choosing your cloud-compute resources, visit Choosing a Server Image and Resources</p>"},{"location":"compute/manage_software/","title":"Managing Software","text":"<p>The LEAPHub environment provides multiple ways to install and manage software. Choose the method that best fits your needs.</p>"},{"location":"compute/manage_software/#quick-installs-temporary","title":"Quick Installs (Temporary)","text":"<p>When you just need software for your current notebook session, you can temporarily install packages using one of the following methods. The lifetime of the installation lasts until the kernel stops. After this, it has to be re-installed.</p> <p>Note</p> <p>Each shell command listed can also be run inside an active Jupyter Notebook by prefixing them with an exclamation mark (i.e. running <code>!pip install numpy</code> in a code cell).</p> <p>1. Pip</p> <p>You can install Python libraries published on Pypi with: <code>pip install &lt;package-name&gt;</code></p> <p>2. Conda / Mamba</p> <p>If you prefer conda/mamba or have non-python dependencies you can install packages with: <code>mamba install &lt;package&gt;</code></p> <p>3. Script</p> <p>If you are installing the same packages every time you start a new session, you can add the install commands in a script. This is especially useful for workshops or group work.</p> <p>Create a .sh file with the following skeletal code:</p> <pre><code>#!/bin/bash\nconda install -y &lt;package&gt; &lt;package&gt; \npip install &lt;package&gt;\necho \"Environment ready!\"\n</code></pre> <p>Then in the terminal run the following code to make the script executable: <code>chmod +x setup-env.sh</code></p> <p>You can now execute the script in your terminal by the following: <code>./&lt;filename&gt;.sh</code></p>"},{"location":"compute/manage_software/#persistent-installs-docker-image","title":"Persistent Installs (Docker Image)","text":"<p>In some cases, a custom Docker image is required to ensure that software packages are installed persistently across kernel restarts. Unlike runtime installations, which are temporary and must be re-run each session, a custom image guarantees that packages are available every time a user starts a new server.</p> <p>Use a custom image for the following scenarios:</p> <ul> <li>When multiple users need the same libraries</li> <li>When setting up workshops, bootcamps, or other workflows</li> <li>When packages take a long time to install or have complex dependencies</li> <li>When Jupyter extensions must be pre-built to function properly (ex: AmazonQ, classic labextensions, etc)</li> </ul> <p>If you find yourself installing the same packages repeatedly, or if runtime installs are unreliable or time-consuming, consider moving to a custom image for a more stable and efficient experience.</p> <p>For instructions on creating and managing a custom image, see the 2i2c hub-user-image guide</p>"},{"location":"compute/manage_software/#how-to-use-a-custom-image-in-the-leap-hub","title":"How to use a custom image in the LEAP Hub:","text":"<ol> <li> <p>Go to the Start Server page in JupyterHub.     (include link to screenshot here)</p> </li> <li> <p>Select Other... from the Image options dropdown. Then enter the link to the custom image in the form of <code>&lt;registry&gt;/&lt;username&gt;/&lt;repo_name&gt;:&lt;git-commit-hash&gt;</code>     (include link to screenshot here)</p> </li> <li> <p>Launch the server - your packages will be pre-installed.</p> </li> </ol>"},{"location":"compute/vs_code_to_hub/","title":"Accessing the Hub via VSCode running locally","text":"<p>Accessing the LEAP JupyterHub through VS Code in your browser offers a powerful, IDE-like experience with minimal setup \u2014 ideal for users working on code-heavy projects or managing full repositories in a unified remote environment.</p>"},{"location":"compute/vs_code_to_hub/#pre-requisites","title":"Pre-requisites","text":"<ul> <li> <p>Register for a LEAP account to use the The hub. Details can be found on the Introduction / Getting Started page.</p> </li> <li> <p>Install websocat on your local machine.     <code>pip install websocat</code> works on Mac OS, and pre-built binaries are available     for all other operating systems.</p> </li> <li> <p>Install VSCode on your local machine.</p> </li> <li> <p>In VSCode, find and install the <code>Remote-SSH</code> and <code>Jupyter</code> extensions from the VSCode Extensions Marketplace.     Remote-SSH will be used to connect to your LEAP Pangeo server, while the Jupyter extension will be used to open .ipynb notebooks and manage the kernel that runs your code on the remote server.</p> </li> </ul> <p></p> <p></p>"},{"location":"compute/vs_code_to_hub/#authentication-options","title":"Authentication Options","text":"<p>Note</p> <p>There are two levels of authentication \u2013 your JupyterHub token and your SSH key pair (public/private). This guide will walk you through both so you can connect with JupyterHub from your VSCode without entering a password.</p>"},{"location":"compute/vs_code_to_hub/#jupyterhub-token","title":"JupyterHub token","text":"<p>The JupyterHub token is like a temporary password that lets your local machine talk to the JupyterHub server over a special channel (via websocat).</p> <p>The JupyterHub token will be included in your <code>~/.ssh/config</code> file on your local machine so ssh knows how to talk to the server.</p>"},{"location":"compute/vs_code_to_hub/#ssh-key-pair","title":"SSH key pair","text":"<p>Your SSH key pair is like a secure ID card. Your local machine holds the private key (secret), and the server holds your public key (the badge that proves it trusts you).</p> <p>The SSH public key will be added to your GitHub account for convenience, and then downloaded onto the JupyterHub server (saved into <code>~/.ssh/authorized_keys</code>) so the server knows your local machine is trusted.</p>"},{"location":"compute/vs_code_to_hub/#setup","title":"Setup","text":""},{"location":"compute/vs_code_to_hub/#1-web-browser-start-your-server","title":"1 [Web Browser] Start your server","text":"<p>This setup only works after you start your JupyterHub server. So, start your server! Please keep in mind that even after setting up Remote-SSH on your VSCode, you would have to start your server on LEAP Pangeo before attempting to connect to the server.</p>"},{"location":"compute/vs_code_to_hub/#2-jupyterhub-token-setup","title":"2 JupyterHub Token Setup","text":""},{"location":"compute/vs_code_to_hub/#21-web-browser-obtain-a-jupyterhub-token","title":"2.1 [Web Browser] Obtain a JupyterHub Token","text":"<p>We will need to create a JupyterHub token for authentication.</p> <ul> <li> <p>Go to the JupyterHub control panel. You can access it via <code>File -&gt; Hub control panel</code> in     JupyterLab, or directly going to https://leap.2i2c.cloud/hub/token.</p> </li> <li> <p>In the top bar, select Token.</p> </li> <li> <p>Create a new Token, and keep it safe. Treat this like you would treat a password to your     JupyterHub instance! It is recommended you set an expiry date for this.</p> </li> </ul>"},{"location":"compute/vs_code_to_hub/#21-local-terminal-configure-local-sshconfig-with-the-jupyterhub-token","title":"2.1 [Local Terminal] Configure local <code>~/.ssh/config</code> with the JupyterHub Token","text":"<p>We will set up our ssh config file to tell <code>ssh</code> how to connect to our JupyterHub. Add an entry that looks like this to the end of your <code>~/.ssh/config</code> file (create it if it does not exist).</p> <pre><code>Host leap.2i2c.cloud\n    User jovyan\n    ProxyCommand websocat --binary -H='Authorization: token &lt;YOUR-JUPYTERHUB-TOKEN&gt;' asyncstdio: wss://%h/user/&lt;YOUR-JUPYTERHUB-USERNAME&gt;/sshd/\n</code></pre> <p>replace:</p> <ul> <li><code>&lt;YOUR-JUPYTERHUB-TOKEN&gt;</code> with the token you generated earlier</li> <li><code>&lt;YOUR-JUPYTERHUB-USERNAME&gt;</code> with your jupyterhub username</li> </ul>"},{"location":"compute/vs_code_to_hub/#3-ssh-key-pair-setup","title":"3 SSH Key Pair Setup","text":""},{"location":"compute/vs_code_to_hub/#31-local-terminal-generate-a-ssh-key-pair","title":"3.1 [Local Terminal] Generate a ssh key pair","text":"<ul> <li> <p>Generate the key pair on local terminal by executing the command:</p> <p><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"</code> Make sure to replace your_email@example.com with your actual email address.</p> </li> <li> <p>Add key to ssh-agent by executing:     <code>eval \"$(ssh-agent -s)\"</code> and</p> <p><code>ssh-add  #~/.ssh/id_ed25519</code></p> </li> <li> <p>Copy the public key:     Use following command to show the public key on your local machine:     <code>cat ~/.ssh/id_ed25519.pub</code>     And copy the entire key, including the email address.</p> <p>Do not expose your private key <code>id_ed25519</code> - the private key should not be shared.</p> </li> </ul>"},{"location":"compute/vs_code_to_hub/#32-web-browser-save-the-public-key-on-github","title":"3.2 [Web Browser] Save the public key on Github","text":"<ul> <li> <p>Go to Github key settings, then click on \"New SSH Key.\"</p> </li> <li> <p>Paste the copied public key (including email address) into the \"Key\" box, and save. In the next step, we will leverage Github to save your public key on JupyterHub.</p> </li> </ul>"},{"location":"compute/vs_code_to_hub/#33-web-browser-setup-ssh-keys-on-your-jupyterhub-server","title":"3.3 [Web Browser] Setup ssh keys on your JupyterHub server","text":"<ul> <li> <p>Make sure your JupyterHub server is still running. If not, start a new one.</p> </li> <li> <p>Open a terminal in JupyterLab on your web browser</p> </li> <li> <p>Run the following commands:</p> <pre><code>mkdir -p ~/.ssh\nwget https://github.com/&lt;YOUR-GITHUB-USERNAME&gt;.keys -O ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n</code></pre> <p>replacing <code>&lt;YOUR-GITHUB-USERNAME&gt;</code> with your github username.</p> </li> <li> <p>Verify that authorized_keys contains your key by running <code>cat ~/.ssh/authorized_keys</code> on the JupyterLab Terminal</p> </li> <li> <p>If it is empty or nonexistent, then create the file authorized_keys in the ~/.ssh/ directory and paste your ssh public key (<code>cat ~/.ssh/id_ed25519.pub</code>, from local terminal) and save file.</p> </li> </ul> <p>With that, we are ready to go!</p>"},{"location":"compute/vs_code_to_hub/#34-local-terminal-try-ssh-ing-into-your-jupyterhub","title":"3.4 [Local Terminal] Try <code>ssh</code>-ing into your JupyterHub!","text":"<p>After all this is setup, you're now able to ssh in! On your local terminal, try:</p> <pre><code>ssh leap.2i2c.cloud\n</code></pre> <p>and it should just work! If configured correctly, <code>leap.2i2c.cloud</code> should not ask you for a password.</p> <p>!!! warning Debugging Help:</p> <p>If you get the error <code>ssh: connect to host leap.2i2c.cloud port 22: Operation timed out</code>, then check you have installed websocat by running <code>run pip install websocat</code> and confirm that ther config file in the correct directory, <code>~/.ssh/config</code></p> <p>If the CLI asks for a password, please verify that your access token and public keys are valid and consistent across platforms and try the previous steps again. Keep in mind this test has to only work once, and it is not necessary to ssh into JupyterHub via CLI once you confirm this works once.</p>"},{"location":"compute/vs_code_to_hub/#4-vscode-connect-to-leap-pangeo-on-vscode","title":"4 [VSCode] Connect to LEAP Pangeo on VSCode","text":"<p>Launch VSCode, click on the top search bar to open the Command Palette (cmd + shift + P on Mac)</p> <ul> <li>Enter <code>&gt;Remote-SSH: Add New SSh Host</code></li> </ul> <p></p> <ul> <li>Enter <code>ssh leap.2i2c.cloud</code></li> </ul> <p></p> <ul> <li>Click on the first option, /Users/\\/.ssh/config <p></p> <ul> <li>Follow further prompts until you are connected. You should see this on your top bar once you are connected.</li> </ul> <p></p> <ul> <li>Click on \"Open Folder\" and select the home directory. You should be able to access your LEAP-Pangeo Notebooks remotely now.</li> </ul> <p></p>"},{"location":"compute/vs_code_to_hub/#5-vscode-write-execute-your-code-via-a-remote-kernel","title":"5 [VSCode] Write / execute your code via a remote kernel","text":"<ul> <li>Once you open a Notebook, you will find a selection menu for a kernel on the top-right.</li> </ul> <ul> <li>Select the option \"Python Environments\"</li> </ul> <ul> <li>Selected the recommended <code>conda (Python 3.x.x)</code> kernel</li> </ul> <p>You should now be good to execute code via remote kernel on the LEAP server from local VSCode!</p>"},{"location":"compute/vs_code_to_hub/#6-vscode-re-connecting-to-remote-ssh-after-setup","title":"6 [VSCode] Re-connecting to Remote SSH After Setup","text":"<ul> <li>Type <code>&gt;Remote-SSH: Connect to Host</code> into the top search bar.</li> </ul> <ul> <li>Select <code>leap.2i2c.cloud</code>. Keep in mind this action will only work if the 2i2c Server is already active.</li> </ul> <ul> <li>Repeat the steps from \"Write / execute your code via a remote kernel\"</li> </ul> <p>Contributors to this documentation:</p> <ul> <li>Yuvi Panda, jupyter-sshd-proxy:</li> <li>Joe Ko, clarifications on how to setup the local public key</li> <li>Sungjoon Park for compiling this workflow for LEAP Pangeo.</li> </ul>"},{"location":"compute/vscode_inside_hub/","title":"Accessing the LEAP Hub via VS Code (in your browser)","text":"<p>The LEAP JupyterHub includes a browser-based version of Visual Studio Code, allowing you to use a full-featured IDE directly in your web browser, without any local installations.</p> <p>This version of VS Code runs on the same remote server as your Jupyter kernel and filesystem \u2014 so any code or terminal work is happening in the cloud, on LEAP compute infrastructure.</p>"},{"location":"compute/vscode_inside_hub/#how-to-launch-vs-code-in-your-browser","title":"How to Launch VS Code in Your Browser","text":"<ol> <li> <p>Log into the LEAP Hub</p> <ul> <li>Go to the LEAP JupyterHub browser and sign in using your institutional credentials.</li> </ul> </li> <li> <p>Start your server (if it's not already running)</p> <ul> <li>Choose a server configuration and click Start.</li> </ul> </li> <li> <p>Click the VS Code Icon</p> <ul> <li>Once your Jupyter environment launches, look in the Launcher tab</li> <li>Find and click the VS Code icon (it should look like the following screenshot).     </li> <li>This opens the in-browser VS Code interface in a new tab or within the same window.</li> </ul> </li> <li> <p>Using VS Code in the Browser</p> <ul> <li>You can open folders, edit files, and run terminals just like in the desktop app.</li> <li>Use the file explorer on the left to navigate your directory.</li> <li>Open a terminal: <code>Terminal</code> &gt; <code>New Terminal</code> (runs in the same environment as Jupyter).</li> <li>Start or open notebooks: click <code>.ipynb</code> files and they\u2019ll open using the built-in Jupyter extension.</li> <li>Git features, syntax highlighting, search, multi-file editing, etc. are all supported.</li> </ul> </li> </ol>"},{"location":"compute/vscode_inside_hub/#tips","title":"Tips","text":"<ul> <li>Python environment: Your Python interpreter is automatically set to the remote Hub environment.</li> <li>Installing packages: Use the built-in terminal and <code>pip install</code> or <code>mamba install</code>. These packages be added to your base environment.</li> <li>Save your work: Changes persist on the Hub\u2019s file system \u2014 no need to worry about syncing with your local machine.</li> </ul>"},{"location":"compute/vscode_inside_hub/#common-issues","title":"Common Issues","text":"<ul> <li>Missing extensions: Some VS Code Marketplace extensions may not be available. This is a limitation of the server-based VS Code.</li> <li>Lag: If your internet connection is slow, performance may be less smooth than the desktop version.</li> <li>VS Code doesn\u2019t launch? Try restarting your Jupyter server, or clear your browser cache and reload.</li> </ul>"},{"location":"data/","title":"Data Overview","text":"<p>LEAP makes use of LOTS of data.</p> <p>This section explains how data is formatted, stored, transferred, and ultimately shared within LEAP.</p> <ul> <li>File Formats describe which formats to use (Zarr, NetCDF, etc.) for cloud-ready, reproducible workflows.</li> <li>Data Locations discusses the different areas data can be stored depending on the stage of work.</li> <li>The Data Lifecycle shows the big picture: ingestion, management, and cleanup.</li> <li>Data Tools explains the tools for moving data between systems, including <code>fsspec/gcsfs</code> and <code>rclone</code>.</li> <li>The Data Catalog describes how to discover datasets once they\u2019ve been published with traceable provenance.</li> </ul>"},{"location":"data/data_catalog/","title":"LEAP Data Catalog","text":"<p>LEAP maintains a data catalog at https://catalog.leap.columbia.edu.</p> <p>The catalog includes data that's been ingested from public sources as well as datasets produced by LEAP. The catalog is publicly accessible.</p>"},{"location":"data/data_catalog/#data-accessability","title":"Data accessability","text":"<p>LEAP data is either stored on a Google Cloud Storage (GCS) or on an Open Storage Network (OSN) pod. Data hosted on GCS is only accessible through the LEAP authenticated Jupyter-hub. Data hosted on the OSN pod is publicly accessible and has no egress cost.</p>"},{"location":"data/data_catalog/#data-viewer","title":"Data Viewer","text":"<p>Zarr CF compliant datasets stored in the LEAP data catalog should be able to be viewed in the browser through the data viewer. This is an experimental feature that allows you to view Zarr data directly in the browser. An example of this can be seen here.</p>"},{"location":"data/data_lifecycle/","title":"Data Lifecycle","text":"<p>This page is meant to serve as a high level reference to the data lifecycle. Please see also the overview of our primary storage locations and their use cases. Troubleshooting of any issues will inevitably involve the specifics of your data. Questions can be addressed to the Data and Compute Team.</p>"},{"location":"data/data_lifecycle/#data-ingestion","title":"Data ingestion","text":"<p>Working on the JupyterHub often requires transferring whatever data you want to work with into the cloud. The term \"data ingestion\" refers to a reproducible way to download and transform data into performant formats like Zarr. Data in LEAP's cloud storage is available for the entire LEAP community and can be added to the LEAP Data Catalog.</p> <p>The Data and Compute team can help with large or complicated data ingestion efforts. We organize such efforts in Github repositories called \"feedstocks\" that centralize knowledge for each request. Once you've verified that the data you need isn't available through the catalog:</p> <ol> <li>Let the LEAP community and the Data and Computation Team know about the new dataset via the 'leap-stc/data_management' issue tracker. You should check existing issues with the tag 'dataset' to see if somebody else might have already requested this particular dataset. If that is not the case you can add a new dataset_request.</li> <li>Use our feedstock template to create a feedstock repository, holding the code and other information needed for ingestion, by following instructions in the README to get you started with either one of the above.</li> </ol> <p>Most tasks fall into one of two categories:</p>"},{"location":"data/data_lifecycle/#1-migrating-public-data","title":"1. Migrating Public Data","text":"<p>LEAP scientsts frequently want to work with publicly accessible data available in a data store like Zenodo, NASA, or NOAA. This is the use case for which much of our tooling is designed.</p> <p>If the data is available via HTTP, one powerful method to avoid duplication is to create a Virtual Zarr Store (see file formats).</p> <p>Sometimes, climate data is published online but not publicly accessible, i.e. it requires some sort of credentials / authorization to access. Even if this is the case, programmatic access is often supported by tooling from the data providers. For example, the Copernicus data ecosystem has their own API and dataset licenses. User-specific access credentials or tokens stored on user directories are not accessible to other members of the hub; they remain private. However, they can be viewed by LEAP Data and Compute admins; if this is a security issue, please consult us for alternative ingestion options.</p>"},{"location":"data/data_lifecycle/#2-migrating-hpc-data","title":"2. Migrating HPC Data","text":"<p>LEAP scientists also have access to an HPC or external filesystem on which their data is being generated. Such data might be ingested to allow collaboration with others in LEAP and/or to apply LEAP computational resources to analysis.</p> <p>If the data is located behind a firewall on an HPC center, the normal 'pull' based paradigm of our feedstocks will not work. In this case we have an option to 'push' the data to a special \"inbox\" bucket (<code>'leap-pangeo-inbox'</code>) on the OSN Pod. Once the data is in this accessible intermediate staging area, the process and tools documented on the feedstock template ought to work.</p> <p>The biggest barrier here is external authentication; see the Authentication section. Once authenticated, users can initiate a data transfer from their 'local' machine (laptop, server, or HPC Cluster) with the tools documented in Data Tools.</p>"},{"location":"data/data_lifecycle/#transferring-data-at-a-glance","title":"Transferring Data (at a glance)","text":"<p>Use this quick guide; full details and examples are in Data Tools.</p> <ul> <li>Small (KB\u2013100s MB): <code>fsspec/gcsfs</code> in Python (great for quick reads/writes and xarray/zarr I/O).</li> <li>Medium/Large (GBs+): <code>rclone</code> (robust, resumable, ideal for bucket\u2194bucket or local\u2194bucket syncing).</li> <li>Direct GCS users: <code>gsutil</code> via the GCloud SDK is available if you\u2019re already in that ecosystem.</li> </ul>"},{"location":"data/data_lifecycle/#deleting-data","title":"Deleting Data","text":"<p>Cleaning up data is an essential part of the lifecycle. Regularly removing unneeded files helps conserve storage resources and control costs for the Hub.</p> <p>Warning</p> <p>Depending on which cloud bucket you are working, make sure to double check which files you are deleting by inspecting the contents and only working in a subdirectory with your username (e.g. <code>gs://&lt;leap-bucket&gt;/&lt;your-username&gt;/some/project/structure</code>)</p> <p>You can remove single files by using a gcsfs/fsspec filessytem as above:</p> <pre><code>import gcsfs\n\nfs = gcsfs.GCSFileSystem()  # equivalent to fsspec.fs('gs')\nfs.rm(\"leap-persistent/funky-user/file_to_delete.nc\")\n</code></pre> <p>If you want to remove zarr stores (which are an \u2018exploded\u2019 data format, and thus represented by a folder structure) you have to recursively delete the store:</p> <p><code>fs.rm(\"leap-scratch/funky-user/processed_store.zarr\", recursive=True)</code></p>"},{"location":"data/data_lifecycle/#scratch-vs-persistent-vs-osn","title":"Scratch vs. Persistent vs. OSN","text":"<ul> <li><code>gs://leap-scratch (scratch):</code> Auto-deletes after 7 days. You can also remove earlier via the methods above.</li> <li><code>gs://leap-persistent (persistent):</code> Manually delete what you no longer need to free space.</li> <li>OSN pod: Coordinate deletions with the Data and Compute Team (open a request before removing large datasets).</li> </ul>"},{"location":"data/data_locations/","title":"Where Data Lives","text":"<p>Data can live in one of three main places:</p> <ul> <li> <p>LEAP's JupyterHub, which includes:</p> <ul> <li>Shared \"cloud buckets\", one for data being actively processed and another for data with longer lifetimes.</li> <li>User directories for very small amount of data and code used privately.</li> </ul> </li> <li> <p>LEAP-owned cloud storage (our OSN pod, co-owned with the M2LINES project)</p> </li> <li> <p>External or HPC Filesystem (private, local)</p> </li> </ul> <p>As a very general guideline:</p> <ul> <li> <p>Data being imported might arrive from external sources (e.g. a large numerical simulation on an HPC system)</p> </li> <li> <p>Data being processed actively during analytical science workflows normally lives in the LEAP Cloud Buckets associated with the Hub.</p> </li> <li> <p>Data being shared with third parties normally lives in the OSN Pod, linked to the data catalog.</p> </li> </ul>"},{"location":"data/data_locations/#leap-cloud-buckets","title":"LEAP Cloud Buckets","text":"<p>Tip</p> <p>Google Cloud's costs are structured to make it free and easy to move data into the buckets, but there are high egress fees for taking data out of GCP Infrastructure. Taking data out means both writing from GCP to outside and reading from an external source like an HPC.</p> <p>LEAP's JupyterHub provides users two cloud buckets in which to store data.</p> <ul> <li><code>gs://leap-scratch/</code> - Temporary Storage deleted after 7 days. Use this bucket for testing and storing large intermediate results. leap-scratch is also a great staging area to use while ingesting data to some other permanent location.</li> <li><code>gs://leap-persistent/</code> - Persistent Storage. Use this bucket for storing results you want to share with other members or access consistently from the Hub.</li> </ul> <p>Files stored on each of those buckets can be accessed by all LEAP members, so be conscious in the way you use these.</p> <ul> <li>Do not put sensitive information (passwords, keys, personal data) into these buckets!</li> <li>When writing to buckets only ever write to your personal folder! Your personal folder is a combination of the bucketname and your github username (e.g. `gs://leap-persistent/your-user-name/').</li> </ul> <p>The JupyterHub is automatically authenticated to read from any of these buckets. See Authentication for details on how to access buckets from 'outside' the JupyterHub.</p> <p>GCS is where to put data if:</p> <ul> <li>You want to move data from your JupyterHub home directory to the cloud.</li> <li>You don't need the data to be accessed outside of the JupyterHub.</li> <li>This data is a work-in-progress and might be regenerated or modified as you do your science.</li> </ul> <p>Tip</p> <p>Use the LEAP GCS buckets when you are actively doing science using the JupyterHub. If you wish to share some kind of finished product with the world, it is best to \"publish\" the data by moving outside GCS into OSN or Zenodo.</p>"},{"location":"data/data_locations/#your-jupyterhub-user-directory","title":"Your JupyterHub User Directory","text":"<p>When you open your hub, you can navigate to the file browser and see all the files in your user directory</p> <p></p> <p>Your user directory behaves like a typical UNIX filesystem. If you save a file from a notebook, you will see it appear in the file browser (you might have to wait a few seconds or press refresh). You can also use the terminal to navigate and manipulate files as you would on your local machine.</p> <p></p> <p>Note</p> <p>Unlike most systems, on a JupyterHub every user sees <code>'/home/jovyan'</code> as their root directory, but the functionality is similar. These are your own files and they cannot be seen/modified by anyone else (except admins).</p> <p>Your home directory is intended only for notebooks, analysis scripts, and small datasets (&lt; 1 GB). Large datasets should stored in cloud buckets. Please try to keep your home directory size to 25GB. To check how much space you are using in your home directory open a terminal window on the hub and run <code>du -h --max-depth=1 ~/ | sort -h</code>. See the Hub Usage Alert for guidance on reducing storage.</p> <p>Warning</p> <p>Home directories have a hard limit of 100GB, which may decrease without warning.</p>"},{"location":"data/data_locations/#leap-cloud-buckets_1","title":"LEAP Cloud Buckets","text":"<p>LEAP uses two Google Cloud buckets (leap-persistent and leap-scratch) and also has an allocation of storage on an Open Storage Network (OSN) pod.</p> <ol> <li>For \"internal\" use, i.e. during the research process, use the JupyterHub in conjunction with dedicated LEAP buckets in Google Cloud Storage (GCS).</li> <li>For publishing or sharing data externally (i.e. in conjunction with a released paper), we recommend pushing to OSN Pods.</li> </ol> <p>Note</p> <p>A common workflow is for data to be private --&gt; get ingested to LEAP GCS --&gt; OSN Pods for publication.</p>"},{"location":"data/data_locations/#open-storage-network","title":"Open Storage Network","text":"<p>The Open Storage Network is distributed cloud storage for the research community. LEAP helped the M2LINES project buy a ~1 petabyte OSN pod. OSN allows s3-like cloud storage that has no egress fees, which means that you can share data with the public or outside colaborators without any cost per request! The downsides are that storage is large but finite, and that authentication is not as seamless as the GCS buckets.</p> <p>The pod is divided into projects and buckets. A project can have multiple buckets. There are currently 2 principal Projects on the Pod (as well as a shared bucket with the m2lines project):</p> <ul> <li><code>'leap-pangeo'</code>: Used for data ingestion across the m2lines and LEAP community<ul> <li>Buckets:<ul> <li><code>'leap-pangeo-inbox'</code>: Write access can be shared with users who want to add data e.g. from an HPC center</li> <li><code>'leap-pangeo-manual'</code>: No write access for users</li> <li><code>'leap-pangeo-pipeline'</code>: No write access for users</li> </ul> </li> </ul> </li> <li><code>'leap'</code>: Used for project data and publications from the LEAP project<ul> <li>Buckets:<ul> <li><code>'leap-pubs'</code>: No write access for users</li> <li>... various project buckets</li> </ul> </li> </ul> </li> </ul> <p>Data can be transferred from <code>leap-pangeo-inbox</code> to <code>leap-pangeo-manual</code> with this rclone github action.</p> <p>OSN is where data should live if if:</p> <ul> <li>You want your data to be publicly accessible outside of the Jupyter-Hub.</li> <li>You need to move data from your Jupyter-Hub home directory to more persistent storage.</li> </ul> <p>To migrate data to OSN, please contact the data-and-compute team on slack. They will contact the OSN pod admin and share bucket credentials for the <code>'leap-pangeo-inbox'</code> bucket. More details are provided under authentication.</p>"},{"location":"data/data_locations/#summary-osn-vs-gcs","title":"Summary - OSN vs GCS","text":"<p>GCS:</p> <ul> <li>You want to move data from your JupyterHub home directory to the cloud.</li> <li>You don't need the data to be accessed outside of the JupyterHub.</li> <li>This data is a work-in-progress and might be regenerated or modified.</li> </ul> <p>OSN:</p> <ul> <li>You want your data to be publicly accessible outside of the Jupyter-Hub.</li> <li>You need to move data from your Jupyter-Hub home directory to more persistent storage.</li> </ul>"},{"location":"data/data_locations/#private-storage-hpc-or-external-filesystems","title":"Private Storage - HPC or External Filesystems","text":"<p>There are scenarios in which it probably does not make sense to migrate your data into the cloud!</p> <ul> <li>If you have a powerful HPC system that produces extremely large volumes (petabytes) of data for processing, the LEAP infrastructure is currently not equipped to handle this.</li> </ul>"},{"location":"data/data_tools/","title":"Data Tools","text":"<p>There are many tools available to interact with cloud object storage. We currently have basic operations documented for three tools:</p> <ul> <li> <p>rclone which provides a Command Line Interface to many different storage backends, see here for more details. Rclone is highly versatile and suits almost all use cases. Details on authentication and usage can be found here.</p> </li> <li> <p>GCloud SDK. One can interact with Google Cloud storage directly using the Google Cloud SDK and Command Line Interface. Please consult the Install Instructions for more guidance.</p> </li> <li> <p>fsspec (and its submodules gcsfs and s3fs) provide filesystem-like access to local, remote, and embedded file systems from within a python session. Fsspec is also used by xarray under the hood and so integrates easily with normal coding workflows (and tools like Dask).</p> <ul> <li><code>fsspec</code> can be used to transfer small amounts of data to google cloud storage or OSN. If you have more than a few hundred MB's, it is worth using Rclone</li> <li><code>fsspec</code> should be installed by default on the Base Pangeo Notebook environment on the JupyterHub. If you are working locally, you can install it with <code>pip or conda/mamba</code>. ex: <code>pip install fsspec</code>.</li> </ul> </li> </ul>"},{"location":"data/data_tools/#tool-selection","title":"Tool Selection","text":"<p>small: KB's to 100's of MB</p> <p>medium: 1 to \\&lt;100GB</p> <p>large: &gt;100GB</p>"},{"location":"data/data_tools/#move-data-from-user-directory-to-gcs","title":"Move data from User Directory to GCS","text":"<ul> <li>Data volume:<ul> <li>small: <code>fsspec/gcsfs</code></li> <li>medium: <code>rclone</code></li> </ul> </li> </ul>"},{"location":"data/data_tools/#move-data-from-jupyterhub-to-osn","title":"Move data from JupyterHub to OSN","text":"<ul> <li>If the data volume is:<ul> <li>small: <code>rclone</code></li> <li>medium: <code>rclone</code></li> </ul> </li> </ul>"},{"location":"data/data_tools/#move-data-from-laptophpc-to-osn","title":"Move data from Laptop/HPC to OSN","text":"<ul> <li>If the data volume is:<ul> <li>small: <code>rclone</code></li> <li>medium: <code>rclone</code></li> <li>large: <code>rclone</code></li> </ul> </li> </ul>"},{"location":"data/data_tools/#rclone","title":"Rclone","text":"<p>Rclone is an open-source command line tool for moving and syncing data. It can be very useful for moving data into or out of LEAP cloud buckets. There is a bit of a learning-curve, but it has extensive docs. Please reach out to us if you run into issues.</p>"},{"location":"data/data_tools/#usage","title":"Usage","text":"<p>Rclone can be installed on the hub by typing in the Terminal:</p> <pre><code>mamba install rclone -y\n</code></pre> <p>Once installed, you can setup \"remotes\", which are storage locations that have credentials. You can view and modify the rclone config file to add remotes. Rclone will show you where this is located by running: <code>rclone config file</code>. Please consult our Authentication guide for instructions on how to setup various remotes on rclone.</p> <pre><code>[leap-pubs]\ntype = s3\nprovider = Ceph\naccess_key_id = &lt;CONTACT DATA AND COMPUTE TEAM&gt;\nsecret_access_key = &lt;CONTACT DATA AND COMPUTE TEAM&gt;\nendpoint = https://nyu1.osn.mghpcc.org\nno_check_bucket = true\n\n[leap-inbox]\ntype = s3\nprovider = Ceph\naccess_key_id = &lt;CONTACT DATA AND COMPUTE TEAM&gt;\nsecret_access_key = &lt;CONTACT DATA AND COMPUTE TEAM&gt;\nendpoint = https://nyu1.osn.mghpcc.org\nno_check_bucket = true\n\n[leap-gcs]\ntype = google cloud storage\nobject_acl = bucketOwnerFullControl\nlocation = us\nenv_auth = true\n</code></pre> <p>Details for the LEAP OSN pods are here.</p> <p>You can find more great documentation, specifically on how to use OSN resources, in this section of the HyTEST Docs</p>"},{"location":"data/data_tools/#listing-directory-contents","title":"Listing directory contents","text":"<p>Rclone has file-system like <code>ls</code> syntax for cloud storage. You can list the files in a bucket/directory with:</p> <pre><code>rclone ls leap-inbox:leap-pangeo-inbox/example-dataset\n</code></pre>"},{"location":"data/data_tools/#moving-data","title":"Moving Data","text":"<p>Rclone can copy data from your local computer, from an HPC system, or between cloud buckets.</p> <p>From Local/HPC to OSN:</p> <pre><code>rclone copy path/to/local_or_local/dir/ leap-inbox:&lt;leap-pangeo-inbox/dataset_name/dataset_input_files\n</code></pre> <p>Between Cloud Buckets:</p> <pre><code>rclone copy \\\n&lt;remote_name_a&gt;:&lt;bucket-name&gt;/dataset-name/dataset-files \\\n&lt;remote_name_b&gt;:&lt;bucket-name&gt;/dataset-name/dataset-files \\\n</code></pre> <p>From Local/HPC to GCS:</p> <pre><code>rclone copy /path/to/local_ir_hpc/data/ \\\n  leap-gcs:leap-persistent/username/my_dataset\n</code></pre> <p>The rclone syntax for copying single files is slightly different then for multiple files. To copy a single file you can use the copyto command. Another option is to copy the containing folder and use the <code>--include</code> or <code>--exclude</code> flags to select the file to copy.</p> <p>Note</p> <p>Transfer speed is likely limited by the internet connection of your local machine.</p>"},{"location":"data/data_tools/#fsspecgcsfs","title":"fsspec/gcsfs","text":"<p>fsspec provides a unified Python interface to local and remote filesystems.</p> <ul> <li>gcsfs is the backend for Google Cloud Storage (GCS).</li> <li>s3fs is the backend for S3-compatible systems, such as the OSN pod.</li> </ul> <p>fsspec integrates directly with scientific Python libraries like xarray, zarr, and dask, which makes it the most seamless option for small transfers or direct read/write access from code.</p>"},{"location":"data/data_tools/#installation","title":"Installation","text":"<p>fsspec and gcsfs should already be available in the Base Pangeo Notebook environment on the JupyterHub.\\ If working locally, install with pip or conda/mamba:</p> <pre><code>pip install fsspec gcsfs s3fs\n# or\nmamba install fsspec gcsfs s3fs -c conda-forge\n</code></pre>"},{"location":"data/data_tools/#usage_1","title":"Usage","text":"<p>Writing to GCS:</p> <pre><code>import fsspec\n\nfs = fsspec.filesystem(\"gcs\")  # uses gcsfs under the hood\nwith fs.open(\"gs://leap-scratch/username/test.txt\", \"w\") as f:\n    f.write(\"hello world\")\n</code></pre> <p>Listing contents in GCS: <code>fs.ls(\"gs://leap-scratch/username/\")</code></p> <p>Writing to OSN (via S3 interface):</p> <pre><code>import fsspec\n\nfs = fsspec.filesystem(\n    \"s3\",\n    key=\"&lt;ACCESS_KEY&gt;\",\n    secret=\"&lt;SECRET_KEY&gt;\",\n    client_kwargs={\"endpoint_url\": \"https://nyu1.osn.mghpcc.org\"},\n)\n</code></pre>"},{"location":"data/data_tools/#moving-data_1","title":"Moving Data","text":"<p>Fsspec is great for copying smaller files from your JupyterHub home directory (/home/jovyan/...) into a LEAP GCS bucket (<code>gs://leap-scratch/...</code> or <code>gs://leap-persistent/...</code>)</p> <p>Copy a single file:</p> <pre><code>import fsspec\nimport shutil\n\n# Local file in your JupyterHub home directory\nlocal_path = \"/home/jovyan/my_project/data.csv\"\n\n# Remote file path in GCS (your user subdirectory)\nremote_path = \"gs://leap-scratch/your-username/data.csv\"\n\n# Create a GCS filesystem object\nfs = fsspec.filesystem(\"gcs\")\n\n# Open the source file and the destination file, then copy\nwith open(local_path, \"rb\") as src, fs.open(remote_path, \"wb\") as dst:\n    shutil.copyfileobj(src, dst)\n</code></pre> <p>Copy an entire directory (e.g., project folder):</p> <pre><code>import fsspec\nimport shutil\nimport os\n\nfs = fsspec.filesystem(\"gcs\")\n\nlocal_dir = \"/home/jovyan/my_project/\"\nremote_dir = \"gs://leap-persistent/your-username/my_project/\"\n\nfor root, _, files in os.walk(local_dir):\n    for fname in files:\n        local_file = os.path.join(root, fname)\n        rel_path = os.path.relpath(local_file, local_dir)\n        remote_file = remote_dir + rel_path\n\n        with open(local_file, \"rb\") as src, fs.open(remote_file, \"wb\") as dst:\n            shutil.copyfileobj(src, dst)\n</code></pre>"},{"location":"data/file_formats/","title":"File Formats","text":"<p>This page explains some of the most common data formats used in the LEAP ecosystem. For guidance on where data should be stored and how to move it into the right location, see Where Data Lives and Data Tools.</p>"},{"location":"data/file_formats/#zarr","title":"Zarr","text":"<p>Zarr is a specification for chunked, compressed, N-dimensional array data that can be stored in object storage (cloud buckets) or on local filesystems. Zarr enables parallel and scalable access to very large datasets. Unlike archival file formats such as NetCDF, the metadata is stored separately in human-readable formats like json. This enables you to access the metadata and understand the structure of the dataset without loading the data.</p>"},{"location":"data/file_formats/#versions","title":"Versions","text":"<p>There are two currently used versions, Zarr V2 and Zarr V3. In early 2025 <code>zarr-python</code>, the python library for Zarr, released a version that supports the Zarr V3 spec. This new spec brings performance improvements and makes working with extensions much easier. The current version of <code>zarr-python</code> on <code>pypi</code> and <code>conda-forge</code> is V3. <code>zarr-python</code> V3 can still read and write V2 data.</p>"},{"location":"data/file_formats/#example","title":"Example","text":"<pre><code>import xarray as xr\n\n# Read in Xarray tutoral data\nds = xr.tutorial.open_dataset(\"air_temperature\", chunks={})\n\n# Write Zarr to GCP with Xarray\nzarr_format = 3  # You can specify version 2 or version 3 here.\nds.to_zarr(\"gs://leap-scratch/username/air_temperature.zarr\", zarr_format=zarr_format)\n\n# Read Zarr with Xarray\nroundtrip_ds = xr.open_zarr(\"gs://leap-scratch/username/air_temperature.zarr\")\nprint(roundtrip_ds)\n</code></pre>"},{"location":"data/file_formats/#virtual-zarr","title":"Virtual Zarr","text":"<p>A massive amount of weather and climate data exists in \"archival\" file formats such as NetCDF, GRIB, HDF, TIFF etc. If these files are accessible over http, you can use the library VirtualiZarr to create Virtual Zarr stores. This allows you to get Zarr-like access speed, without duplicating the data!</p>"},{"location":"data/file_formats/#example_1","title":"Example","text":"<p>For detailed examples check out the usage page on the VirtualiZarr docs.</p>"},{"location":"data/file_formats/#icechunk","title":"Icechunk","text":"<p>Icechunk is an open-source transactional version of Zarr. This allows you to version control your Zarr data, create branches, safely incrementally update your data and much more. The core Icechunk library is written in rust and has very performant I/O. You can read about it and see examples on the Icechunk docs.</p> <p>Think of it like \"git for zarr data\": you can create, version, and tag your data.</p>"},{"location":"data/file_formats/#example_2","title":"Example","text":"<p>The Icechunk docs have tons of useage examples and tutorials. An Xarray specific guide can be found here.</p>"},{"location":"data/file_formats/#netcdf","title":"NetCDF","text":"<p>Network Common Data Form (NetCDF) is one of the most established formats for storing scientific climate and weather data. It is widely used for model outputs and observational datasets because it is self-describing, portable, and supported by most scientific tools. While NetCDF is a super common and stable format, it is not ideal for working with in the cloud. A nice description of why can be found in this blog.</p>"},{"location":"data/file_formats/#example-in-python","title":"Example in Python:","text":"<pre><code>import xarray as xr\n\n# Read in Xarray tutoral data\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\n# Write NetCDF\nds.to_netcdf(\"air_temperature.nc\")\n\n# Roundtrip\nds = xr.open_dataset(\"air_temperature.nc\")\n\nprint(ds)\n</code></pre>"},{"location":"edu/","title":"Using LEAP Resources for Education and Broadening Participation","text":"<p>LEAP data and computing resources can be used to support</p> <ul> <li>Classes occuring over many months</li> <li>Bootcamps and Summer Schools occuring over a week or two.</li> </ul>"},{"location":"edu/bootcamps/","title":"Running Bootcamps","text":"<p>Bootcamp materials in the LEAP-Pangeo Bootcamp Repository. Please add the materials you develop for your bootcamp to this repo as described below.</p>"},{"location":"edu/bootcamps/#preparing-for-bootcamps","title":"Preparing for Bootcamps","text":"<p>Past attendees have suggested that it is very helpful to have the instructor write code from scratch. This ensures the following:</p> <ul> <li>The lecturer takes substantially more time to write live, ensuring that others can follow along.</li> <li>The lecturer is not a superhuman, but also makes mistakes and has to debug code. This is a very important lesson for students to learn, as it is a very common part of the coding process and often not shown in lectures.</li> <li>It also allows the lecturer to explain the code as they write it, which is a very important part of the learning process.</li> </ul> <p>If it is necessary to use a notebook pre-populated with code, please clear all outputs prior to starting the lecture by following this procedure:</p> <ul> <li>In the Jupyter notebook, click on <code>Edit</code> in the top taskbar.</li> <li>Find and click <code>Clear Outputs of All Cells</code> from the dropdown menu.</li> <li>Save Jupyter notebook with cleared outputs by clicking <code>Save</code> from the top taskbar and then clicking <code>Save Notebook</code>.</li> <li>After the lecture is over, save the Jupyter notebook again to save the complete version with all outputs.</li> </ul>"},{"location":"edu/bootcamps/#materials-for-the-bootcamp","title":"Materials for the bootcamp","text":"<p>We encourage you to reuse materials from past bootcamps and adapt them to your needs. If you re-use material from previous bootcamps, please don't modify the original notebooks, instead create new copies for your use.</p> <ul> <li>Create a new folder in the \"Codes\" and \"Lectures\" folders and add your materials there. If you are giving the same material as a previous bootcamp, please copy them.</li> </ul> <p>Note</p> <p>If you are using materials from other sources, like the An Introduction to Earth and Environmental Data Science book. Please just follow the past events structure and link to the original materials.</p> <ul> <li>README.md Enter a new entry under the \"Events\" section. This should include an nbgitpuller link for each notebook you work with, so participants can easily pull the materials to their hub (there is a neat tool to generate these links). Please also use the LEAP-Pangeo Hub badge by adding code like this:</li> </ul> <pre><code>[![Open in LEAP-Pangeo Hub](https://custom-icon-badges.demolab.com/badge/Jupyter%20Hub-Launch%20%F0%9F%9A%80-blue?style=for-the-badge&amp;logo=leap-globe)](&lt;generated_link&gt;)\n</code></pre>"},{"location":"edu/bootcamps/#running-the-bootcamp","title":"Running the bootcamp","text":""},{"location":"edu/bootcamps/#prepare-ahead-of-the-event","title":"Prepare ahead of the event","text":"<p>Bootcamp instructors should make sure that they are added to the Bootcamp Instructors Github Team by contacting the Data and Compute Team. This ensures that the people running the bootcamp can perform tasks that might be needed the day of (mostly often signing up folks who either entered wrong information or did not register ahead of time).</p> <p>Before the event, we recommend that instructors follow these steps ahead of time:</p> <ul> <li>Before the bootcamp begins, double check that all participants github names are present in the leap-pangeo-base-access team! If folks are in this team they will have access to the hub. If they are not present here, then we need to troubleshoot Hub access.</li> <li>The first thing to try is manually triggering the \"Parse Member Sheet and Add New Members\" action by clicking on <code>Run Workflow &gt; Run Workflow</code> in the top right menu. If a student has filled out the hub access sign-up form with the correct information, this action will invite them to the team and once they accept the invite, they will receive access to the Hub.</li> <li>If this does not work, it is likely that the student either (1 - Pending Invite) have not accepted the invite to the team yet or (2 - Wrong Username) made a mistake while inputting their Github username on the Hub access request form.<ul> <li>For (1 - Pending Invite), ask the student to accept the leap-pangeo-base-access team invite on Github. More details on troubleshooting can be found here.</li> <li>For (2 - Wrong Username), check their github handles in the Member Data Google Sheet for typos.</li> </ul> </li> <li>In either case, it is useful to have view/edit access to the Member Data Google Sheet. LEAPs Managing Director has the ability to add data or give others edit access \u2013 request access via Slack if you need it.</li> </ul> <p>If you are anticipating many attendees to access the hub at the same time to follow along, it is useful to consult with the Data &amp; Compute Team beforehand to ensure a smooth presentation.</p>"},{"location":"edu/bootcamps/#troubleshooting-during-the-event","title":"Troubleshooting during the event","text":"<p>If students have trouble signing in to the hub, please refer to Sign-up Troubleshooting for troubleshooting steps.</p>"},{"location":"edu/bootcamps/#after-the-event","title":"After the event","text":"<p>Please add your versions of the filled notebooks to a subfolder named <code>filled_notebooks</code> in the event folder you created above, so participants and future instructors can access them later.</p>"},{"location":"edu/classes/","title":"Teaching Classes","text":"<p>LEAP provides infrastructure support to LEAP-Affiliated Courses.</p> <p>Note</p> <p>If you are launching a new course that relies on the LEAP Infrastructure, please let us know: leap@columbia.edu</p>"},{"location":"edu/classes/#preparing-to-teach","title":"Preparing to teach","text":""},{"location":"edu/classes/#timing","title":"Timing","text":"<p>If you are an instructor of a LEAP-Affiliated Course and want your students to the LEAP Jupyter-hub, contact the Data &amp; Compute Team at least 3 weeks before the class start date in order to coordinate efforts for onboarding.</p>"},{"location":"edu/classes/#class-material","title":"Class Material","text":"<p>We suggest that instructors and teaching assistants prepare class material directly on the hub as much as possible to ensure an identical experience for the students during the class.</p> <p>To develop, edit, and test class material, we suggest that instructors themselves apply for membership under the education category and check out our Getting Started guide.</p>"},{"location":"edu/classes/#sign-up-students","title":"Sign up students","text":"<p>All students should apply for membership under the education category at least two weeks before the starting date of the class.</p> <p>Note</p> <p>In order to gain access to the Jupyter-hub, a Github account is required. It is free to create one. Please instruct your students to create one before requesting access.</p> <p>Instructors can verify that students have access by checking that their github usernames as listed as members of leap-pangeo-base-access on Github. We suggest that instructors direct students to the Getting Started page and ask them to test their access to the Hub before the class starts to avoid any technical interruptions.</p> <p>We recommend familiarizing yourself with the troubleshooting steps below before the start of the class, so that you are prepared to sign up students on the first day if necessary.</p>"},{"location":"edu/classes/#troubleshooting","title":"Troubleshooting","text":""},{"location":"edu/classes/#students-cannot-access-jupyterhub","title":"Students cannot access JupyterHub","text":"<p>Refer students to sign-up troubleshooting if problems persist. If this does not resolve the issue, please contact the Data &amp; Compute Team.</p>"},{"location":"introduction/","title":"Overview and First Steps","text":"<p>These pages detail the use of LEAP's computational and data resources.</p> <ul> <li>Get oriented to the available data and computing resources</li> <li>Get started by arranging access to the resources</li> </ul>"},{"location":"introduction/getting_started/","title":"Getting Started","text":""},{"location":"introduction/getting_started/#gaining-access","title":"Gaining Access","text":"<p>To gain access to the Hub please apply via our Application Form. Access to the JupyterHub is implemented via the leap-stc GitHub organization. Each membership tier is associated with a Github Team and determines the resources available to the user.</p> Tier Github Team Resources Available Intended for PUBLIC leap-pangeo-public-access Access to limited performance notebooks, read only access to all LEAP storage, and write access to our scratch bucket, where everything is deleted after 7 days. Participants in the wider LEAP community (e.g., public outreach, newsletter subscribers). EDUCATION leap-pangeo-base-access Access to storage and computing resource up to 4 cores and 32GB RAM on JupyterHub servers. Participants in LEAP Education programs (e.g., Bootcamp, academic courses, and summer programs). RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. Researchers who have been referred by a LEAP scientist. Six month renewal application required. LEAP-FUNDED RESEARCH leap-pangeo-full-access Access to storage and computing resource up to 16 cores and 128GB RAM on JupyterHub servers + GPU options. Researchers receiving LEAP funding. <p>Once you have applied for membership, it will take a few days until you can be approved. Please watch out for email from LEAP that contains instructions on how to proceed.</p> <p>Note</p> <p>It is very common for users to not realize that they have already received an invitation to join the <code>leap-stc</code> Github Organization! If you are unsure whether your membership has gone through, please consult the support section</p>"},{"location":"introduction/getting_started/#logging-in","title":"Logging In","text":"<ol> <li>Navigate to https://leap.2i2c.cloud/ and click the Log in to continue button.</li> <li>You will be prompted to authorize a GitHub application. Say yes to everything.     You must belong to the appropriate GitHub team in order to access the hub.</li> <li>You will redirected to a screen with server configuration options. Several drop down menus enable users to choose their environment image (default is Base Pangeo Notebook\") and compute resources (CPU, GPU if enabled).     </li> <li>After clicking Start, wait for your server to start up. It can take up to few minutes.</li> </ol> <p>Note</p> <p>Depending on your [membership] tier you might see different options (e.g. GPU might be hidden).</p>"},{"location":"introduction/getting_started/#resource-sizing","title":"Resource sizing","text":"<p>When starting a VM on the JupyterHub you have multiple configuration options.</p> <ul> <li> <p>Software environment (\"Image\") - Which software environment to use. The default, Base Pangeo Notebook contains many commonly used earth science python libraries. You can also supply your own custom docker image.</p> </li> <li> <p>VM size (\"Node share\") - How many cpu cores and amount of memory.</p> </li> <li> <p>VM type - CPU or GPU focused VM.</p> </li> </ul> <p>Note</p> <p>\u26a0\ufe0fThe GPU images should be used only when needed to accelerate model training.</p> <p>If unsure which options to choose, check out our guide on scaling compute.</p>"},{"location":"introduction/getting_started/#jupyterlab","title":"JupyterLab","text":"<p>After your server fires up, you will be dropped into a JupyterLab environment. From here you can open your own shell terminals, write scripts, and create iPython Notebooks! You also have seamless access to multiple cloud storage options adjacent to your compute resources.</p> <p>If you are new to JupyterLab, you might want to peruse the user guide.</p>"},{"location":"introduction/what_is_the_hub/","title":"What is LEAP JupyterHub?","text":"<p>LEAP's primary data and computational resources are available through the LEAP JupyterHub. The JupyterHub is a shared \"cloud-based\" computing environment running on cloud-based storage and compute resources.</p> <p>The hub is designed primarily around interactive use, however long running jobs are also possible.</p> <p>To gain access to the Hub please see the registration page. LEAP's JupyterHub is managed by our partner 2i2c.</p> <p>The Hub includes</p> <ul> <li> <p>computing resources accessible via a web browser or VSCode as described on this page. The resources include a variety of hardware configurations and a range of pre-defined or custom software environments.</p> </li> <li> <p>data resources divided among generous shared \"cloud buckets\" for data and limited home directories for scripts etc. as described at Where Data Lives.</p> </li> </ul> <p>The computing resources have fast access to LEAP's data resources; they also have fast connections to the broader internet which lowers the barrier to working with data held elsewhere.</p> <p>Compared to using:</p> <ul> <li>a laptop, the Hub offers fast access to data and the ability to access much more powerful computational resources including GPUs for ML training tasks.</li> <li>HPCs, clusters, etc. the hub offers simpler access and less competition for resources (since the pool from which the resources are drawn is enormous). It does not, however, easily support non-interactive use.</li> </ul> <p>Moving data in to data resources is free; moving data out is expensive (see Data Lifecycle). Please plan your workflow accordingly.</p>"},{"location":"reference/","title":"Technical Details","text":"<p>These pages have the nitty-gritty details on how to accomplish specific tasks with the LEAP JupyterHub.</p>"},{"location":"reference/authentication/","title":"Authentication","text":""},{"location":"reference/authentication/#accessing-leap-cloud-buckets","title":"Accessing LEAP Cloud Buckets","text":"<p>If you are not working on the LEAP Jupyterhub, i.e. a local machine or HPC, accessing the LEAP cloud data requires authentication. We manage bucket access directly via Google Cloud Console (GCS), on a per-user basis.</p> <ol> <li>Ensure your computer can access the Google Cloud SDK via its Terminal. Please consult the Install Instructions for guidance if you do not have gcloud on your machine. Login to whichever google account you want LEAP to grant access to with the gcloud auth login command. To ensure everything worked, verify that <code>gcloud auth list</code> returns the correct account.</li> <li>Reach out to the Data and Compute team to request access. We'll need to know which account email you have logged into gcloud with as well as which bucket you need to access. Permissions will be granted based on what kind of access is needed (read-only, write, etc). Once confirmed on our end, verify which permissions were granted with <code>gcloud storage buckets get-iam-policy gs://&lt;bucket-name&gt;</code> under your account email.</li> <li>Once you have access, you have a variety of options for actually moving the data; most tools or libraries have some way of syncing with gcloud. You may find it helpful to generate Application Default Credentials using the <code>gcloud auth application-default login</code> command. This helps applications automatically find credentials at runtime, bypassing the need for manual authentication. This will create a OAuth 2.0 Token file in some location like <code>$HOME/gcloud/application_default_credentials.json</code>.</li> </ol> <p>For most use cases, rclone gets the job done.</p>"},{"location":"reference/authentication/#configuring-rclone-with-google-cloud","title":"Configuring Rclone with Google Cloud","text":"<ol> <li>Install rclone with <code>conda install rclone</code></li> <li>Run rclone config and follow the directives for the google cloud storage setup. Generally the default options for good when in doubt, but this part can be tricky! Feel free to reach out with questions if unsure. Some guidelines:<ol> <li>Since IAM access directly granted, you can leave the 'Project number` blank.</li> <li>We do not anonymous access, that ruins the whole purpose of granting access! Put <code>false</code> when prompted.</li> <li>object_acl specifies the default permissions for any new objects you upload to storage. We recommend choosing either 5 or 6. bucket_acl does not matter since you're unlikely to create any new cloud buckets.</li> <li>If you followed the steps above and generated Application Default Credentials, you can choose <code>true</code> (which is NOT the default) for the \"env_auth\" option, which tells rclone to get GCP IAM credentials from runtime or as needed. If your <code>gcloud auth login</code> session is valid, it will be used to authenticate rclone without much hassle! Especially if you are on an HPC without a web browser, this is the best option.</li> </ol> </li> <li>Upon completion, you will have created or added to your rclone configuration file. To find out the location of your config file, you can run <code>rclone config file</code> (it is usually something like <code>$HOME/.config/rclone/rclone.conf</code>). If you see the remote that was just set up, you can now use rclone freely! See our reference on rclone for guidance.</li> </ol>"},{"location":"reference/authentication/#osn-pod-authentication","title":"OSN Pod Authentication","text":"<p>Step by Step instructions</p> <ol> <li>Reach out to the the data-and-compute team. They can share bucket credentials for the <code>'leap-pangeo-inbox'</code> bucket.</li> <li>How you exactly achieve the upload will depend on your preference. Some common options include:<ul> <li>Load / Aggregate your NetCDF files using xarray and save the output to Zarr using <code>.to_zarr(...)</code></li> <li>Use fsspec or rclone to move an existing zarr store to the target bucket</li> </ul> </li> </ol> <p>Whatever method you use, the final upload must contain one or more valid Zarr stores. These are required for compatibility with LEAP's cloud-optimized workflows. A typical workflow for the above steps might look like:</p> <pre><code>import xarray as xr\nimport zarr\nfrom obstore.store import S3Store\nfrom zarr.storage import ObjectStore\n\n# Optional: This will start a dask distributed client for parallel processing.\nclient = Client()\nprint(client.scheduler.address)\n\nds = xr.tutorial.open_dataset(\"air_temperature\", chunks={})\nds_processed = ds.mean(...).resample(...)  # sample modifications to data\n\n# define our credentials, bucket name and dataset path\nDATASET_NAME = \"&lt;INSERT_YOUR_DATASET_NAME_HERE&gt;\"\n\nosnstore = S3Store(\n    \"leap-pangeo-inbox\",\n    prefix=f\"{DATASET_NAME}/{DATASET_NAME}.zarr\",\n    aws_endpoint=\"https://nyu1.osn.mghpcc.org\",\n    access_key_id=\"&lt;ASK LEAP DCT MEMBERS FOR CREDENTIALS&gt;\",\n    secret_access_key=\"&lt;ASK LEAP DCT MEMBERS FOR CREDENTIALS&gt;\",\n    client_options={\"allow_http\": True},\n)\nzstore = ObjectStore(osnstore)\n\n# Write our dataset as Zarr to OSN\nds.to_zarr(\n    zstore,\n    zarr_format=3,\n    consolidated=False,\n    mode=\"w\",\n)\n\n# Note: Your data can be read anywhere, by anyone!\nroundtrip = xr.open_zarr(\n    \"https://nyu1.osn.mghpcc.org/leap-pangeo-inbox/dataset-name/dataset-name.zarr\",\n    consolidated=False,\n)\n</code></pre>"},{"location":"reference/infrastructure_details/","title":"Infrastructure Details","text":""},{"location":"reference/infrastructure_details/#leap-jupyterhub","title":"LEAP JupyterHub","text":"Hub Address https://leap.2i2c.cloud/ Hub Location Google Cloud <code>us-central1</code> Hub Operator 2i2c Hub Configuration https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/leap"},{"location":"reference/infrastructure_details/#leap-pangeo-data-catalog","title":"LEAP-Pangeo Data Catalog","text":"Catalog Address catalog.leap.columbia.edu Management Repo https://github.com/leap-stc/data-management Maintained in collaboration with Carbonplan <p>For more explanation about ingesting data and linking it into the catalog, see Data Lifecycles.</p>"},{"location":"support/contact/","title":"Contact","text":"<p>The preferred way to contact the Data &amp; Compute Team is via the LEAP Slack Workspace. The steps to take are listed below:</p>"},{"location":"support/contact/#ask-a-question-on-the-leap-slack-workspace","title":"Ask a Question on the LEAP Slack Workspace","text":"<ul> <li>Request access to the LEAP Slack by contacting LEAP: leap@columbia.edu</li> <li>Once you are given access, add yourself to relevant channels listed below within the workspace</li> <li>Search these two channels to see if anyone else had similar questions - chances are, the solution to your questions are already here!</li> <li>If issue persists after trying previous solutions, then let us know by posting a new thread and tagging us at <code>@data-and-compute</code>!</li> </ul> <p><code>leap-pangeo</code>: channel for non-technical support for LEAP-Pangeo (Questions pertaining to Hub access, Education, etc.)</p> <p><code>technical_questions</code>: channel for community-wide support on technical issues</p>"},{"location":"support/dask_killed_workers/","title":"Dask \"Killed Workers\"","text":"<p>The \"Killed Worker\" message in dask can result due to a variety of reasons. Here are some of the common reasons why you might see such an error message</p>"},{"location":"support/dask_killed_workers/#datasets-chunks-too-large","title":"Datasets Chunks too large","text":"<p>Issue</p> <p>The default dask worker configuration can deal well with dataset chunk sizes of ~100MB. If the chunks of your data are significantly larger, your worker might crash just upon loading a few of these chunks into memory.</p> <p>Solution</p> <p>You can change the configuration of your dask workers and increase the memory each worker has to deal with larger chunks. You can adjust the memory by passing additional options to the dask-gatway cluster upon creation:</p> <pre><code>from dask_gateway import Gateway\n\ngateway = Gateway()\noptions = gateway.cluster_options()\n\noptions.worker_memory = 10  # 10 GB of memory per worker.\n\n# Create a cluster with those options\ncluster = gateway.new_cluster(options)\ncluster.scale(...)\nclient = cluster.get_client()\n</code></pre>"},{"location":"support/hub_access_error/","title":"Hub Access Error","text":""},{"location":"support/hub_access_error/#i-cannot-log-into-the-hub","title":"I cannot log into the hub \ud83d\ude31","text":"<p>If you are unable to log into the hub, please check the following steps:</p> <ul> <li>[ ] Check if you are member of the appropriate github teams.</li> </ul> <p>If you are not, follow these steps:</p> <ul> <li>[ ] Did you sign up for LEAP membership?</li> <li>[ ] Did you receive a github invite? See Sign-up Troubleshooting for how to check for that.</li> <li>[ ] Check again if you are part of the appropriate github teams (most commonly <code>leap-pangeo-base-access</code>).</li> </ul> <p>If you are a member of one of the github teams, try the following steps:</p> <ul> <li>[ ] Refresh the browser cache</li> <li>[ ] Try a different browser</li> <li>[ ] Restart your local machine</li> <li>[ ] Try accessing the hub</li> </ul> <p>If these steps do not work, please reach out to the Data and Compute Team.</p>"},{"location":"support/hub_usage_alert/","title":"Hub Usage Alert","text":""},{"location":"support/hub_usage_alert/#i-received-a-warning-about-space-on-my-user-directory","title":"I received a warning about space on my User Directory","text":"<p>If you get a Hub Usage Alert email, this means you are violating the user directory storage limit. Remember that user directories are for scripts and notebooks not datasets! Users who persistently violate hub usage policies may get their cloud access temporarily restricted.</p>"},{"location":"support/hub_usage_alert/#how-do-i-manage-my-disk-space","title":"How do I manage my disk space?","text":"<ul> <li>To see which files and directories are taking up the bulk of your storage, run <code>du -h --max-depth=1 ~/ | sort -h</code> in Terminal. It will likely reveal cached files and small/medium size data files that can be removed without disrupting typical usage.</li> <li>Delete cached files, ipython checkpoints, and any other unwanted files.</li> <li>If you still require more storage, it is likely that you are storing downloaded data in your user directory. We recommend storing data in a LEAP cloud bucket or data catalog. For more information, please consult our data locations.</li> </ul> <p>Our goal is to accomodate all community members, so we are happy to assist users in relocating data. If you have any concerns, please reach out to the Data and Compute Team.</p>"},{"location":"support/signup_troubleshooting/","title":"Sign-up Troubleshooting","text":"<p>Before we begin, please ensure you have filled out the LEAP membership form with the correct information.</p>"},{"location":"support/signup_troubleshooting/#where-is-my-github-invite","title":"Where is my Github invite?","text":""},{"location":"support/signup_troubleshooting/#method-1","title":"Method 1","text":"<p>Please check your email account (the one you used to sign up for the Github account you listed in the membership form - independent of the email you use for LEAP) for an invite that looks similar to this:</p> <p></p> <p>Click the link and accept all invites.</p>"},{"location":"support/signup_troubleshooting/#method-2","title":"Method 2","text":"<p>Alternatively, you can log into your Github account, and you will see a notification in the top right menu under the \"Organizations\" tab.</p> <p></p> <p>You can follow that and accept the invitation there aswell.</p> <p></p>"},{"location":"support/signup_troubleshooting/#filled-out-the-form-but-i-did-not-get-an-invite","title":"Filled out the form, but I did not get an invite?","text":"<p>In this case, it is most likely that you made a typo when filling out the membership form. Please fill it out again here, and wait a day before checking your inbox for an invite. If you need access ASAP, please alert your instructor or contact the Data &amp; Compute Team to get access.</p>"}]}